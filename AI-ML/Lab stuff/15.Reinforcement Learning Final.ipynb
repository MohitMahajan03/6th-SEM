{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "172ab586",
   "metadata": {},
   "source": [
    "Q-Learning algorithm for a 3x3 grid world environment. In this environment, the agent learns to navigate from a start state to a goal state.\n",
    "\n",
    "Simple Grid World Environment (3x3)\n",
    "The grid world is a 3x3 grid where:\n",
    "\n",
    "The agent starts at the top-left corner (0, 0).\n",
    "The goal is at the bottom-right corner (2, 2).\n",
    "\n",
    "GridWorld Environment: A simple 3x3 grid world with a start position and a goal position.\n",
    "Q-Table: A 3D array to store Q-values for each state-action pair. \n",
    "The dimensions are [grid_size, grid_size, num_actions].\n",
    "Hyperparameters: Learning rate (alpha), discount factor (gamma), exploration rate (epsilon), and number of training episodes.\n",
    "Training Loop: The agent is trained using the Q-learning algorithm. For each episode, the environment is reset, and the agent explores or exploits based on the epsilon-greedy policy. The Q-values are updated using the Q-learning formula.\n",
    "Evaluation: The trained agent is evaluated in the grid world, and the path taken by the agent is rendered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a2132",
   "metadata": {},
   "source": [
    "Reinforcement learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward. Unlike supervised learning, where the model is trained on a dataset with labeled examples, reinforcement learning involves learning from the consequences of actions, exploring and exploiting to find the best strategies.\n",
    "\n",
    "Key Concepts in Reinforcement Learning\n",
    "Agent: The learner or decision-maker.\n",
    "Environment: Everything the agent interacts with.\n",
    "State: A representation of the current situation of the environment.\n",
    "Action: Choices made by the agent that affect the state.\n",
    "Reward: Feedback from the environment based on the action taken.\n",
    "Policy: The strategy that the agent employs to determine actions based on states.\n",
    "Value Function: A prediction of future rewards used to evaluate the desirability of states or actions.\n",
    "Q-Learning: A popular RL algorithm that aims to learn the value of action-reward pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f70fa7",
   "metadata": {},
   "source": [
    "Applications of Reinforcement Learning\n",
    "Robotics: Learning to perform tasks like walking, grasping, and manipulating objects.\n",
    "Gaming: Creating agents that can play and excel in games like Chess, Go, and video games.\n",
    "Autonomous Vehicles: Decision-making for navigation and control.\n",
    "Healthcare: Personalized treatment plans and medical decision-making.\n",
    "Finance: Algorithmic trading and portfolio management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23173795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      "[[[ 0.64053665  0.81        0.60643746  0.69455038]\n",
      "  [ 0.78572591  0.9        -0.28965447  0.67666503]\n",
      "  [ 0.85753237  1.          0.72352471  0.71151964]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.729      -0.34260979  0.57067222  0.60391069]\n",
      "  [ 0.80174241  0.10954271  0.          0.17777732]\n",
      "  [ 0.89747569 -0.091       0.03914909  0.        ]\n",
      "  [ 0.3439      0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.6561      0.31929802  0.57360514  0.55233607]\n",
      "  [-0.14976338  0.51781876  0.03082074  0.059049  ]\n",
      "  [ 0.72838184  0.          0.06476343  0.        ]\n",
      "  [-0.23122     0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the environment\n",
    "grid = [\n",
    "    [0, 0, 0, 1],\n",
    "    [0, -1, 0, -1],\n",
    "    [0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "# Define actions: 0=up, 1=right, 2=down, 3=left\n",
    "actions = 4\n",
    "\n",
    "# Q-table\n",
    "q_table = np.zeros((len(grid), len(grid[0]), actions))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "# Training parameters\n",
    "episodes = 1000\n",
    "\n",
    "def choose_action(state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, 3)  # Explore\n",
    "    else:\n",
    "        return np.argmax(q_table[state[0]][state[1]])  # Exploit\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    if action == 0:  # Up\n",
    "        next_state = [max(state[0] - 1, 0), state[1]]\n",
    "    elif action == 1:  # Right\n",
    "        next_state = [state[0], min(state[1] + 1, len(grid[0]) - 1)]\n",
    "    elif action == 2:  # Down\n",
    "        next_state = [min(state[0] + 1, len(grid) - 1), state[1]]\n",
    "    else:  # Left\n",
    "        next_state = [state[0], max(state[1] - 1, 0)]\n",
    "    return next_state\n",
    "\n",
    "# Training loop\n",
    "for _ in range(episodes):\n",
    "    state = [2, 0]  # Start state\n",
    "    \n",
    "    while state != [0, 3]:  # Until goal state is reached\n",
    "        action = choose_action(state, epsilon)\n",
    "        next_state = get_next_state(state, action)\n",
    "        reward = grid[next_state[0]][next_state[1]]\n",
    "        \n",
    "        # Q-value update\n",
    "        old_q = q_table[state[0]][state[1]][action]\n",
    "        next_max = np.max(q_table[next_state[0]][next_state[1]])\n",
    "        new_q = (1 - alpha) * old_q + alpha * (reward + gamma * next_max)\n",
    "        q_table[state[0]][state[1]][action] = new_q\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "# Print the learned Q-table\n",
    "print(\"Learned Q-table:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b502cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A . .\n",
      ". . .\n",
      ". . G\n",
      "\n",
      ". A .\n",
      ". . .\n",
      ". . G\n",
      "\n",
      ". . A\n",
      ". . .\n",
      ". . G\n",
      "\n",
      ". . .\n",
      ". . A\n",
      ". . G\n",
      "\n",
      ". . .\n",
      ". . .\n",
      ". . A\n",
      "\n",
      "Goal reached!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the environment\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.size = 3\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (2, 2)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = self.start\n",
    "        return self.agent_pos\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.agent_pos\n",
    "        if action == 0:  # Up\n",
    "            x = max(x - 1, 0)\n",
    "        elif action == 1:  # Down\n",
    "            x = min(x + 1, self.size - 1)\n",
    "        elif action == 2:  # Left\n",
    "            y = max(y - 1, 0)\n",
    "        elif action == 3:  # Right\n",
    "            y = min(y + 1, self.size - 1)\n",
    "\n",
    "        self.agent_pos = (x, y)\n",
    "        if self.agent_pos == self.goal:\n",
    "            return self.agent_pos, 1, True\n",
    "\n",
    "        return self.agent_pos, 0, False\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.zeros((self.size, self.size), dtype=str)\n",
    "        grid[:] = '.'\n",
    "        grid[self.goal] = 'G'\n",
    "        grid[self.agent_pos] = 'A'\n",
    "        print('\\n'.join(' '.join(row) for row in grid))\n",
    "        print()\n",
    "\n",
    "# Initialize the environment\n",
    "env = GridWorld()\n",
    "\n",
    "# Q-Learning parameters\n",
    "q_table = np.zeros((env.size, env.size, 4))\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 1000\n",
    "\n",
    "# Training the agent\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice([0, 1, 2, 3])  # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Exploit learned values\n",
    "        \n",
    "        next_state, reward, done = env.step(action)\n",
    "        old_value = q_table[state][action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "        q_table[state][action] = new_value\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "# Evaluate the agent\n",
    "state = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    state, reward, done = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"Goal reached!\" if reward == 1 else \"Failed to reach the goal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae89eee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
